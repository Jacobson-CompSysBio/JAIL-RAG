{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import wandb\n",
    "from tqdm.notebook import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from importlib import reload\n",
    "from torch_scatter import scatter\n",
    "from transformers import pipeline\n",
    "from IPython.display import clear_output\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import preprocess as pp\n",
    "from utils.evaluate import eval_funcs, normalize\n",
    "from utils.collate import collate_fn \n",
    "from utils.ckpt import _reload_best_model\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "# base_path = '../data/DREAM4_gold_standards/'\n",
    "\n",
    "# c_node_id_data = BiologicalDataset(base_path + 'connections_node_id')\n",
    "# c_node_label_data = BiologicalDataset(base_path + 'connections_node_label')\n",
    "# sp_node_id_data = BiologicalDataset(base_path + 'shortest_path_node_id')\n",
    "# sp_node_label_data = BiologicalDataset(base_path + 'shortest_path_node_label')\n",
    "# get dataset\n",
    "data_path = '../data/subgraphs/all'\n",
    "dataset = BiologicalDataset(data_path)\n",
    "idx_split = dataset.get_idx_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets on idx\n",
    "test_dataset = Subset(dataset, idx_split['test'])\n",
    "\n",
    "# options\n",
    "batch_size = 8\n",
    "\n",
    "# make dataloaders\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c96302f884104a919ae1653d8f5bc2e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model is frozen\n"
     ]
    }
   ],
   "source": [
    "# base model\n",
    "base_graph_llm = GraphLLM(max_text_len=512,\n",
    "                     max_max_new_tokens=32,\n",
    "                     max_memory=[80, 80],\n",
    "                     llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                     llm_frozen=True,\n",
    "                     revision=\"main\") # args are defaulted in the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint from ../checkpoints/graph_llm_fsdp/epoch_1_best.pth.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DGX01/Personal/krusepi/codebase/projects/llms/bio-graph-rag/notebooks/../utils/ckpt.py:64: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(path, map_location=\"cpu\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-15 11:32:55,122] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rag/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/home/pkr/miniconda3/envs/rag/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlvsym'\n",
      "/home/pkr/miniconda3/envs/rag/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlopen'\n",
      "/home/pkr/miniconda3/envs/rag/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlclose'\n",
      "/home/pkr/miniconda3/envs/rag/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlerror'\n",
      "/home/pkr/miniconda3/envs/rag/compiler_compat/ld: /usr/local/cuda/lib64/libcufile.so: undefined reference to `dlsym'\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "# path\n",
    "path = '../checkpoints/graph_llm_fsdp/epoch_1_best.pth'\n",
    "\n",
    "# load model\n",
    "trained_graph_llm = _reload_best_model(base_graph_llm, path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Node Prediction Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60bc0032e33f43e0aba7b1fb5f5f5956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2258 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "Accuracy: 0.00% | 0/8\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "Accuracy: 0.00% | 0/16\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "no\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "\n",
      "\n",
      "yes\n",
      "\n",
      "Incorrect :(\n",
      "\n",
      "Accuracy: 0.00% | 0/24\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# loop through dataloader\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(loader):\n\u001b[0;32m---> 14\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     pred \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     17\u001b[0m     actual \u001b[38;5;241m=\u001b[39m out[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/codebase/projects/llms/bio-graph-rag/notebooks/../utils/graph_llm.py:251\u001b[0m, in \u001b[0;36mGraphLLM.inference\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    245\u001b[0m inputs_embeds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(batch_inputs_embeds, batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    246\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mrnn\u001b[38;5;241m.\u001b[39mpad_sequence(\n\u001b[1;32m    247\u001b[0m     [torch\u001b[38;5;241m.\u001b[39mtensor(x, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m batch_attention_mask],\n\u001b[1;32m    248\u001b[0m     batch_first\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    249\u001b[0m     padding_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    250\u001b[0m )\n\u001b[0;32m--> 251\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    256\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(outputs, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    259\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m: samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpred\u001b[39m\u001b[38;5;124m'\u001b[39m: pred,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m'\u001b[39m: samples[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdesc\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    265\u001b[0m }\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2216\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2220\u001b[0m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2221\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2222\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2223\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/transformers/generation/utils.py:3249\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3247\u001b[0m     probs \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msoftmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3248\u001b[0m     \u001b[38;5;66;03m# TODO (joao): this OP throws \"skipping cudagraphs due to ['incompatible ops']\", find solution\u001b[39;00m\n\u001b[0;32m-> 3249\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmultinomial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m   3250\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3251\u001b[0m     next_tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(next_token_scores, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# options\n",
    "verbose = True\n",
    "model = trained_graph_llm\n",
    "loader = test_loader\n",
    "\n",
    "# set to eval\n",
    "model.model.generation_config.pad_token_id = model.tokenizer.pad_token_id\n",
    "model.eval()\n",
    "\n",
    "n_correct = 0\n",
    "i = 0\n",
    "# loop through dataloader\n",
    "for batch in tqdm(loader):\n",
    "    out = model.inference(batch)\n",
    "\n",
    "    pred = out['pred']\n",
    "    actual = out['label']\n",
    "\n",
    "    # test accuracy\n",
    "    for p, a in zip(pred, actual):\n",
    "        p_ans, p_think = normalize(p)\n",
    "        a = str(a)\n",
    "        if verbose:\n",
    "            print(p_think)\n",
    "            print(p_ans)\n",
    "            print(a)\n",
    "            print()\n",
    "        if a in p_ans:\n",
    "            n_correct += 1\n",
    "            if verbose:\n",
    "                print(\"Correct!\")\n",
    "                print()\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"Incorrect :(\")\n",
    "                print()\n",
    "        i += 1\n",
    "    print(f\"Accuracy: {n_correct/i:.2%} | {n_correct}/{i}\", end='\\r')\n",
    "        \n",
    "acc = n_correct / i\n",
    "print(f\"Accuracy: {acc:.2%} | {n_correct}/{i}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [14364, 26652, 53831, 35767, 52295, 42935, 67547, 87806],\n",
       " 'pred': ['You will be given a biological graph and a question. Provide an answer of YES or NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES or NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES or NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning,',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES or NO based on the question and the given input graph. Explain your reasoning,'],\n",
       " 'label': ['no', 'no', 'no', 'yes', 'no', 'yes', 'yes', 'yes'],\n",
       " 'question': ['Is there an edge between nodes 722 and 108?',\n",
       "  'Is there an edge between nodes 47 and 89?',\n",
       "  'Is there an edge between nodes 376 and 58?',\n",
       "  'Is there an edge between nodes 84 and 149?',\n",
       "  'Is there an edge between nodes 69 and 256?',\n",
       "  'Is there an edge between nodes 86 and 568?',\n",
       "  'Is there an edge between nodes 679 and 230?',\n",
       "  'Is there an edge between nodes 645 and 412?'],\n",
       " 'desc': ['You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Explain your reasoning, and surround it with with <think> and </think> tags. Then provide an answer, surrounded in <answer> and </answer> tags. ']}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference(batch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
