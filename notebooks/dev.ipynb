{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# Basic Python libraries for various operations\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from DGXutils import GetLowestGPU\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face libraries for transformer models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom\n",
    "from utils import preprocess as pp\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset\n",
    "from utils.evaluate import eval_funcs\n",
    "from utils.config import parse_args_llama\n",
    "from utils.ckpt import _save_checkpoint, _reload_best_model\n",
    "from utils.collate import collate_fn\n",
    "from utils.seed import seed_everything\n",
    "from utils.lr_schedule import adjust_learning_rate\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility across Python, NumPy, and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for random number generation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Explanation:\n",
    "        1. Sets seed for Python's built-in random module for basic random operations.\n",
    "        2. Sets seed for NumPy, ensuring consistent random number generation in array operations.\n",
    "        3. Sets seed for PyTorch CPU operations.\n",
    "        4. If CUDA is available, sets seed for all GPU devices.\n",
    "        5. Configures cuDNN to ensure deterministic behavior:\n",
    "           - Sets deterministic flag to True, ensuring reproducible results.\n",
    "           - Disables benchmarking to prevent algorithm selection based on hardware.\n",
    "\n",
    "    Note:\n",
    "        Setting deterministic behavior may impact performance but ensures consistent results\n",
    "        across multiple runs, which is crucial for debugging and research.\n",
    "    \"\"\"\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the function to set random seed for reproducibility\n",
    "set_random_seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\")\n",
    "os.environ[\"WANDB_PROJECT\"] = os.getenv(\"WANDB_PROJECT\")\n",
    "os.environ[\"WANDB_ENTITY\"] = os.getenv(\"WANDB_ENTITY\")\n",
    "\n",
    "# set visible devices to gpus 0-3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 512\n",
    "\n",
    "# get dataset to see what we're working with\n",
    "path = \"../data/subgraphs/all/\"\n",
    "dataset = BiologicalDataset(path)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e44b1658954b988d2e05fd10f4ef68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Lora\n"
     ]
    }
   ],
   "source": [
    "# load model to see what we're working with\n",
    "model = GraphLLM(max_txt_len=T,\n",
    "                max_new_tokens=200,\n",
    "                llm_model_path='meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "                llm_frozen=False, # set frozen to false so we can train with RL\n",
    "                fsdp=False, \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract answer from the model output.\n",
    "\n",
    "    Args:\n",
    "        text (str): The model output text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract answer from prediction\n",
    "    ans = ''.join(re.findall(r\"<answer>(.*?)</answer>\", text)[-1]) \n",
    "    ans = ans.lower() \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a set of examples provided by a PyTorch DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model (GraphLLM): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader yielding evaluation batches.\n",
    "            Each batch is expected to be a dictionary with keys such as 'id', 'question', \n",
    "            'scope', 'label', 'desc', and 'graph'. The values for 'label', 'desc', and 'question'\n",
    "            should be lists (or tensors in the case of labels) of the same batch size.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the evaluation examples.\n",
    "    \n",
    "    References:\n",
    "        - PyTorch DataLoader documentation: https://pytorch.org/docs/stable/data.html\n",
    "        - Accelerate library for device placement and distributed inference: https://huggingface.co/docs/accelerate \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    batch_size = len(batch[\"label\"])\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"EVALUATION ON {batch_size} EXAMPLES\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Perform model inference on the whole batch with no gradient computation.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.inference(batch)\n",
    "\n",
    "    # Assume outputs[\"pred\"] is a list or tensor of predictions of length equal to batch_size.\n",
    "    for i in range(batch_size):\n",
    "        # Extract the predicted answer for this example.\n",
    "        predicted = extract_answer(outputs[\"pred\"][i])\n",
    "        expected = batch[\"label\"][i]\n",
    "        is_correct = (predicted == expected)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        # Print details for this example.\n",
    "        print(\"\\nPrompt:\")\n",
    "        print(batch[\"desc\"][i] + ' ' + batch[\"question\"][i])\n",
    "        print(\"\\nExpected Answer:\")\n",
    "        print(expected)\n",
    "        print(\"\\nExtracted Answer:\")\n",
    "        print(predicted)\n",
    "        print(\"\\nFull Generated Response:\")\n",
    "        # If outputs[\"pred\"] is a tensor or list of strings, print accordingly.\n",
    "        print(outputs[\"pred\"][i])\n",
    "        print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    accuracy = (correct / batch_size) * 100\n",
    "    print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{batch_size})\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Switch model back to training mode after evaluation.\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reward formatting\n",
    "def reward_format(gt, pred):\n",
    "    \"\"\"\n",
    "    if the answer is in the correct format, reward 1.25, else reward -1\n",
    "    \"\"\"\n",
    "    \n",
    "    # answer format\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "\n",
    "    return 1.25 if re.match(pattern, pred, re.DOTALL | re.VERBOSE) else -1\n",
    "\n",
    "# define reward function for node connectivity\n",
    "def reward_correct_yn(gt, pred) -> int: \n",
    "    \"\"\"\n",
    "    given a yes/no answer and ground truth, return 1 if correct, -1 if incorrect\n",
    "    \"\"\"\n",
    "\n",
    "    # extract answer from prediction\n",
    "    ans = ''.join(re.findall(r\"<answer>(.*?)</answer>\", pred)) \n",
    "    ans = ans.lower() \n",
    "\n",
    "    # if the model produced an answer, compare it to the ground truth - return 1 if correct, -1 if incorrect\n",
    "    if ans == gt:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def combined_reward(gt, pred):\n",
    "    \"\"\"\n",
    "    combined reward function for yes/no questions and answer formatting\n",
    "    \"\"\"\n",
    "    return reward_correct_yn(gt, pred) + reward_format(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_log_softmax(logits, input_ids):\n",
    "    \"\"\"\n",
    "    Computes log probabilities for specific tokens in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The raw logits output from the model.\n",
    "        input_ids (torch.Tensor): The token IDs for which we want the log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Applies log softmax to convert logits to log probabilities over the vocabulary.\n",
    "        2. Uses gather to extract only the log probabilities corresponding to the input_ids.\n",
    "        3. Removes the extra dimension to match the original shape of input_ids.\n",
    "    \"\"\"\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def compute_log_probs(model, batch, logits_to_keep):\n",
    "    \"\"\"\n",
    "    Computes the log probabilities for a batch of tokens.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        input_ids (torch.Tensor): Token IDs for input sequences.\n",
    "        attention_mask (torch.Tensor): Attention mask for input sequences.\n",
    "        logits_to_keep (int): Number of tokens to keep from the end of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Gets logits from the model for the input sequence.\n",
    "        2. Selects logits for all tokens except the last one (as we predict next tokens).\n",
    "        3. Selects only the last 'logits_to_keep' tokens from both logits and input_ids.\n",
    "        4. Computes log probabilities for these tokens using selective_log_softmax.\n",
    "    \"\"\"\n",
    "    _, out = model(batch)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    input_ids = input_ids[:, -logits_to_keep:]\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "    \"\"\"\n",
    "    Creates a mask for completion tokens that excludes tokens after the EOS token.\n",
    "\n",
    "    Args:\n",
    "        completion_ids (torch.Tensor): Token IDs of the generated completions.\n",
    "        eos_token_id (int): The ID of the end-of-sequence token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token.\n",
    "\n",
    "    Explanation:\n",
    "        1. Identifies positions where EOS tokens occur in each sequence.\n",
    "        2. Finds the index of the first EOS token in each sequence.\n",
    "        3. Creates a mask where positions before and including the first EOS are 1, others are 0.\n",
    "        4. If no EOS token is found in a sequence, all positions are set to 1.\n",
    "    \"\"\"\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, batch, num_generations=4, max_completion_length=32):\n",
    "    \"\"\"\n",
    "    Generates multiple completions for each prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        prompts (list): List of text prompts.\n",
    "        num_generations (int): Number of completions to generate per prompt.\n",
    "        max_completion_length (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask.\n",
    "\n",
    "    Explanation:\n",
    "        1. Encodes the prompts and moves them to the appropriate device.\n",
    "        2. Repeats each prompt num_generations times to generate multiple completions.\n",
    "        3. Generates completions using the model with specified parameters.\n",
    "        4. Extracts the completion IDs (excluding the prompt tokens).\n",
    "        5. Creates a mask for the completions using create_completion_mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize prompt inputs\n",
    "    prompt_inputs = [batch[\"desc\"][i] + batch[\"question\"][i] for i in range(len(batch[\"desc\"]))]\n",
    "    inputs = model.tokenizer(prompt_inputs, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"]\n",
    "    prompt_mask = inputs[\"attention_mask\"]\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}\")\n",
    "\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "\n",
    "    outputs = model.inference(batch, num_generations=num_generations)\n",
    "\n",
    "    print(f\"Output batch size: {outputs['out_ids'].size(0)}\")\n",
    "    completion_ids = outputs[\"out_ids\"][:, prompt_length:]\n",
    "    completion_mask = create_completion_mask(completion_ids, model.tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [58449],\n",
       " 'question': ['Is there an edge between nodes 239 and 120?'],\n",
       " 'scope': ['all'],\n",
       " 'label': ['no'],\n",
       " 'desc': ['A question with a yes/no answer is provided along with a graph. Answer the question based on the graph. Provide reasoning inside of <think></think> tags and the answer inside of <answer></answer> tags.'],\n",
       " 'graph': DataBatch(x=[250, 1024], edge_index=[2, 2234], num_nodes=250, batch=[250], ptr=[2])}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(loader))\n",
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch size: 1\n",
      "Output batch size: 4\n"
     ]
    }
   ],
   "source": [
    "prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(model, batch, num_generations=4, max_completion_length=32)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
