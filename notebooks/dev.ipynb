{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "# Basic Python libraries for various operations\n",
    "import random\n",
    "import copy\n",
    "import re\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import wandb\n",
    "from dotenv import load_dotenv\n",
    "from DGXutils import GetLowestGPU\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# PyTorch and related libraries for deep learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# Hugging Face libraries for transformer models\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "# custom\n",
    "from utils import preprocess as pp\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset\n",
    "from utils.evaluate import eval_funcs\n",
    "from utils.config import parse_args_llama\n",
    "from utils.ckpt import _save_checkpoint, _reload_best_model\n",
    "from utils.collate import collate_fn\n",
    "from utils.seed import seed_everything\n",
    "from utils.lr_schedule import adjust_learning_rate\n",
    "\n",
    "def set_random_seed(seed: int = 42):\n",
    "    \"\"\"\n",
    "    Set the random seed for reproducibility across Python, NumPy, and PyTorch.\n",
    "\n",
    "    Args:\n",
    "        seed (int): The seed value to use for random number generation.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "\n",
    "    Explanation:\n",
    "        1. Sets seed for Python's built-in random module for basic random operations.\n",
    "        2. Sets seed for NumPy, ensuring consistent random number generation in array operations.\n",
    "        3. Sets seed for PyTorch CPU operations.\n",
    "        4. If CUDA is available, sets seed for all GPU devices.\n",
    "        5. Configures cuDNN to ensure deterministic behavior:\n",
    "           - Sets deterministic flag to True, ensuring reproducible results.\n",
    "           - Disables benchmarking to prevent algorithm selection based on hardware.\n",
    "\n",
    "    Note:\n",
    "        Setting deterministic behavior may impact performance but ensures consistent results\n",
    "        across multiple runs, which is crucial for debugging and research.\n",
    "    \"\"\"\n",
    "    # Set the seed for Python's built-in random module\n",
    "    random.seed(seed)\n",
    "    # Set the seed for NumPy\n",
    "    np.random.seed(seed)\n",
    "    # Set the seed for PyTorch\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    # Ensure deterministic behavior in cuDNN (may impact performance)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call the function to set random seed for reproducibility\n",
    "set_random_seed(42)\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"WANDB_API_KEY\"] = os.getenv(\"WANDB_API_KEY\")\n",
    "os.environ[\"WANDB_PROJECT\"] = os.getenv(\"WANDB_PROJECT\")\n",
    "os.environ[\"WANDB_ENTITY\"] = os.getenv(\"WANDB_ENTITY\")\n",
    "\n",
    "# set visible devices to gpus 0-3\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1,2,3,4,5,6,7\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 512\n",
    "\n",
    "# get dataset to see what we're working with\n",
    "path = \"../data/subgraphs/all/\"\n",
    "dataset = BiologicalDataset(path)\n",
    "loader = DataLoader(dataset, batch_size=1, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9562616e85e74e3f9a54549b372be8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with Lora\n"
     ]
    }
   ],
   "source": [
    "# load model to see what we're working with\n",
    "model = GraphLLM(max_txt_len=T,\n",
    "                max_new_tokens=200,\n",
    "                llm_model_path='meta-llama/Meta-Llama-3.1-8B-Instruct',\n",
    "                llm_frozen=False, # set frozen to false so we can train with RL\n",
    "                fsdp=False, \n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extraction + Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_answer(text):\n",
    "    \"\"\"\n",
    "    Extract answer from the model output.\n",
    "\n",
    "    Args:\n",
    "        text (str): The model output text.\n",
    "    \n",
    "    Returns:\n",
    "        str: The extracted answer.\n",
    "    \"\"\"\n",
    "    \n",
    "    # extract answer from prediction\n",
    "    ans = ''.join(re.findall(r\"<answer>(.*?)</answer>\", text)[-1]) \n",
    "    ans = ans.lower() \n",
    "    return ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the model on a set of examples provided by a PyTorch DataLoader.\n",
    "\n",
    "    Args:\n",
    "        model (GraphLLM): The model to evaluate.\n",
    "        dataloader (torch.utils.data.DataLoader): A DataLoader yielding evaluation batches.\n",
    "            Each batch is expected to be a dictionary with keys such as 'id', 'question', \n",
    "            'scope', 'label', 'desc', and 'graph'. The values for 'label', 'desc', and 'question'\n",
    "            should be lists (or tensors in the case of labels) of the same batch size.\n",
    "\n",
    "    Returns:\n",
    "        float: The accuracy of the model on the evaluation examples.\n",
    "    \n",
    "    References:\n",
    "        - PyTorch DataLoader documentation: https://pytorch.org/docs/stable/data.html\n",
    "        - Accelerate library for device placement and distributed inference: https://huggingface.co/docs/accelerate \n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "\n",
    "    batch_size = len(batch[\"label\"])\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(f\"EVALUATION ON {batch_size} EXAMPLES\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Perform model inference on the whole batch with no gradient computation.\n",
    "    with torch.no_grad():\n",
    "        outputs = model.inference(batch)\n",
    "\n",
    "    # Assume outputs[\"pred\"] is a list or tensor of predictions of length equal to batch_size.\n",
    "    for i in range(batch_size):\n",
    "        # Extract the predicted answer for this example.\n",
    "        predicted = extract_answer(outputs[\"pred\"][i])\n",
    "        expected = batch[\"label\"][i]\n",
    "        is_correct = (predicted == expected)\n",
    "        if is_correct:\n",
    "            correct += 1\n",
    "\n",
    "        # Print details for this example.\n",
    "        print(\"\\nPrompt:\")\n",
    "        print(batch[\"desc\"][i] + ' ' + batch[\"question\"][i])\n",
    "        print(\"\\nExpected Answer:\")\n",
    "        print(expected)\n",
    "        print(\"\\nExtracted Answer:\")\n",
    "        print(predicted)\n",
    "        print(\"\\nFull Generated Response:\")\n",
    "        # If outputs[\"pred\"] is a tensor or list of strings, print accordingly.\n",
    "        print(outputs[\"pred\"][i])\n",
    "        print(\"\\nCorrect:\", \"✓\" if is_correct else \"✗\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "    accuracy = (correct / batch_size) * 100\n",
    "    print(f\"\\nAccuracy: {accuracy:.2f}% ({correct}/{batch_size})\")\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "    # Switch model back to training mode after evaluation.\n",
    "    model.train()\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reward Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to reward formatting\n",
    "def reward_format(gt, pred):\n",
    "    \"\"\"\n",
    "    if the answer is in the correct format, reward 1.25, else reward -1\n",
    "    \"\"\"\n",
    "    \n",
    "    # answer format\n",
    "    pattern = r\"^<think>.*?</think><answer>.*?</answer>$\"\n",
    "\n",
    "    return 1.25 if re.match(pattern, pred, re.DOTALL | re.VERBOSE) else -1\n",
    "\n",
    "# define reward function for node connectivity\n",
    "def reward_correct_yn(gt, pred) -> int: \n",
    "    \"\"\"\n",
    "    given a yes/no answer and ground truth, return 1 if correct, -1 if incorrect\n",
    "    \"\"\"\n",
    "\n",
    "    # extract answer from prediction\n",
    "    ans = ''.join(re.findall(r\"<answer>(.*?)</answer>\", pred)) \n",
    "    ans = ans.lower() \n",
    "\n",
    "    # if the model produced an answer, compare it to the ground truth - return 1 if correct, -1 if incorrect\n",
    "    if ans == gt:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "def combined_reward(gt, pred):\n",
    "    \"\"\"\n",
    "    combined reward function for yes/no questions and answer formatting\n",
    "    \"\"\"\n",
    "    return reward_correct_yn(gt, pred) + reward_format(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GRPO Train Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def selective_log_softmax(logits, input_ids):\n",
    "    \"\"\"\n",
    "    Computes log probabilities for specific tokens in the vocabulary.\n",
    "\n",
    "    Args:\n",
    "        logits (torch.Tensor): The raw logits output from the model.\n",
    "        input_ids (torch.Tensor): The token IDs for which we want the log probabilities.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Applies log softmax to convert logits to log probabilities over the vocabulary.\n",
    "        2. Uses gather to extract only the log probabilities corresponding to the input_ids.\n",
    "        3. Removes the extra dimension to match the original shape of input_ids.\n",
    "    \"\"\"\n",
    "    log_probs = nn.functional.log_softmax(logits, dim=-1)\n",
    "    return log_probs.gather(dim=-1, index=input_ids.unsqueeze(-1)).squeeze(-1)\n",
    "\n",
    "def compute_log_probs(model, batch, logits_to_keep):\n",
    "    \"\"\"\n",
    "    Computes the log probabilities for a batch of tokens.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        input_ids (torch.Tensor): Token IDs for input sequences.\n",
    "        attention_mask (torch.Tensor): Attention mask for input sequences.\n",
    "        logits_to_keep (int): Number of tokens to keep from the end of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Log probabilities of the selected tokens.\n",
    "\n",
    "    Explanation:\n",
    "        1. Gets logits from the model for the input sequence.\n",
    "        2. Selects logits for all tokens except the last one (as we predict next tokens).\n",
    "        3. Selects only the last 'logits_to_keep' tokens from both logits and input_ids.\n",
    "        4. Computes log probabilities for these tokens using selective_log_softmax.\n",
    "    \"\"\"\n",
    "    _, out = model(batch)\n",
    "    logits = out.logits[:, -1, :]\n",
    "    input_ids = input_ids[:, -logits_to_keep:]\n",
    "    logits = logits[:, -logits_to_keep:, :]\n",
    "    return selective_log_softmax(logits, input_ids)\n",
    "\n",
    "def create_completion_mask(completion_ids, eos_token_id):\n",
    "    \"\"\"\n",
    "    Creates a mask for completion tokens that excludes tokens after the EOS token.\n",
    "\n",
    "    Args:\n",
    "        completion_ids (torch.Tensor): Token IDs of the generated completions.\n",
    "        eos_token_id (int): The ID of the end-of-sequence token.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A binary mask with 1s for valid tokens and 0s after the EOS token.\n",
    "\n",
    "    Explanation:\n",
    "        1. Identifies positions where EOS tokens occur in each sequence.\n",
    "        2. Finds the index of the first EOS token in each sequence.\n",
    "        3. Creates a mask where positions before and including the first EOS are 1, others are 0.\n",
    "        4. If no EOS token is found in a sequence, all positions are set to 1.\n",
    "    \"\"\"\n",
    "    is_eos = completion_ids == eos_token_id\n",
    "    eos_idx = torch.full((is_eos.size(0),), is_eos.size(1), dtype=torch.long, device=completion_ids.device)\n",
    "    mask_exists = is_eos.any(dim=1)\n",
    "    eos_idx[mask_exists] = is_eos.int().argmax(dim=1)[mask_exists]\n",
    "    sequence_indices = torch.arange(is_eos.size(1), device=completion_ids.device).expand(is_eos.size(0), -1)\n",
    "    return (sequence_indices <= eos_idx.unsqueeze(1)).int()\n",
    "\n",
    "def generate_completions(model, batch, num_generations=4, max_completion_length=32):\n",
    "    \"\"\"\n",
    "    Generates multiple completions for each prompt.\n",
    "\n",
    "    Args:\n",
    "        model: The language model.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        prompts (list): List of text prompts.\n",
    "        num_generations (int): Number of completions to generate per prompt.\n",
    "        max_completion_length (int): Maximum number of tokens to generate.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Containing prompt IDs, prompt mask, completion IDs, and completion mask.\n",
    "\n",
    "    Explanation:\n",
    "        1. Encodes the prompts and moves them to the appropriate device.\n",
    "        2. Repeats each prompt num_generations times to generate multiple completions.\n",
    "        3. Generates completions using the model with specified parameters.\n",
    "        4. Extracts the completion IDs (excluding the prompt tokens).\n",
    "        5. Creates a mask for the completions using create_completion_mask.\n",
    "    \"\"\"\n",
    "\n",
    "    # tokenize prompt inputs\n",
    "    prompt_inputs = [batch[\"desc\"][i] + batch[\"question\"][i] for i in range(len(batch[\"desc\"]))]\n",
    "    inputs = model.tokenizer(prompt_inputs, return_tensors=\"pt\", padding=True, padding_side=\"left\")\n",
    "    prompt_ids = inputs[\"input_ids\"]\n",
    "    prompt_mask = inputs[\"attention_mask\"]\n",
    "    print(f\"Input batch size: {prompt_ids.size(0)}\")\n",
    "\n",
    "    prompt_length = prompt_ids.size(1)\n",
    "    prompt_ids = prompt_ids.repeat_interleave(num_generations, dim=0)\n",
    "    prompt_mask = prompt_mask.repeat_interleave(num_generations, dim=0)\n",
    "\n",
    "    outputs = model.inference(batch, num_generations=num_generations)\n",
    "\n",
    "    print(f\"Output batch size: {outputs['out_ids'].size(0)}\")\n",
    "    completion_ids = outputs[\"out_ids\"][:, prompt_length:]\n",
    "    completion_mask = create_completion_mask(completion_ids, model.tokenizer.eos_token_id)\n",
    "    return prompt_ids, prompt_mask, completion_ids, completion_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rollout_data(model, ref_model, batch_samples, num_generations, max_completion_length):\n",
    "    \"\"\"\n",
    "    Generates data for GRPO rollouts including completions and log probabilities.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model being trained.\n",
    "        ref_model: The reference model for KL divergence calculation.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        batch_samples (list): Batch of training samples.\n",
    "        num_generations (int): Number of completions to generate per sample.\n",
    "        max_completion_length (int): Maximum completion length.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing all data needed for GRPO updates.\n",
    "\n",
    "    Explanation:\n",
    "        1. Extracts prompts and expected answers from the batch samples.\n",
    "        2. Generates completions using the current policy model.\n",
    "        3. Combines prompt and completion tokens.\n",
    "        4. Computes log probabilities from both the policy model and reference model.\n",
    "        5. Formats completions for reward calculation.\n",
    "        6. Repeats prompts and answers to match the number of generated completions.\n",
    "        7. Returns all data needed for GRPO loss calculation.\n",
    "    \"\"\"\n",
    "    prompts = [sample[\"question\"] if isinstance(sample, dict) else sample[0] for sample in batch_samples]\n",
    "    answers = [sample[\"label\"] if isinstance(sample, dict) else sample[1] for sample in batch_samples]\n",
    "    with torch.no_grad():\n",
    "        prompt_ids, prompt_mask, completion_ids, completion_mask = generate_completions(\n",
    "            model, prompts, num_generations, max_completion_length\n",
    "        )\n",
    "        input_ids = torch.cat([prompt_ids, completion_ids], dim=1)\n",
    "        attention_mask = torch.cat([prompt_mask, completion_mask], dim=1)\n",
    "        logits_to_keep = completion_ids.size(1)\n",
    "        old_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "        ref_log_probs = compute_log_probs(ref_model, input_ids, attention_mask, logits_to_keep)\n",
    "    formatted_completions = [[{'content': model.tokenizer.decode(ids, skip_special_tokens=True)}] for ids in completion_ids]\n",
    "    repeated_prompts = [p for p in prompts for _ in range(num_generations)]\n",
    "    repeated_answers = [a for a in answers for _ in range(num_generations)]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"completion_mask\": completion_mask,\n",
    "        \"old_log_probs\": old_log_probs,\n",
    "        \"ref_log_probs\": ref_log_probs,\n",
    "        \"formatted_completions\": formatted_completions,\n",
    "        \"repeated_prompts\": repeated_prompts,\n",
    "        \"repeated_answers\": repeated_answers,\n",
    "        \"logits_to_keep\": logits_to_keep,\n",
    "        \"batch_size\": len(prompts),\n",
    "        \"num_generations\": num_generations\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grpo_loss(model, ref_model, rollout_data, reward_function, beta=0.01, epsilon=0.2):\n",
    "    \"\"\"\n",
    "    Computes the GRPO loss for updating the policy model.\n",
    "\n",
    "    Args:\n",
    "        model: The policy model being trained.\n",
    "        ref_model: The reference model for KL divergence calculation.\n",
    "        rollout_data (dict): Data generated by generate_rollout_data.\n",
    "        tokenizer: The tokenizer for encoding and decoding text.\n",
    "        reward_function: Function that calculates rewards for completions.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        epsilon (float): Clipping parameter for PPO.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: The GRPO loss to be minimized.\n",
    "\n",
    "    Explanation:\n",
    "        1. Computes current token log probabilities using the policy model.\n",
    "        2. Calculates the probability ratio between current and old policies.\n",
    "        3. Computes rewards using the provided reward_function.\n",
    "        4. Calculates advantages by standardizing rewards within each prompt.\n",
    "        5. Computes the PPO surrogate objective with clipping.\n",
    "        6. Calculates the KL divergence between reference and policy models.\n",
    "        7. Combines surrogate loss and KL penalty.\n",
    "        8. Averages the loss across all tokens and batches.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    input_ids = rollout_data[\"input_ids\"]\n",
    "    attention_mask = rollout_data[\"attention_mask\"]\n",
    "    completion_mask = rollout_data[\"completion_mask\"]\n",
    "    logits_to_keep = rollout_data[\"logits_to_keep\"]\n",
    "    old_log_probs = rollout_data[\"old_log_probs\"]\n",
    "    ref_log_probs = rollout_data[\"ref_log_probs\"]\n",
    "    token_log_probs = compute_log_probs(model, input_ids, attention_mask, logits_to_keep)\n",
    "    ratio = torch.exp(token_log_probs - old_log_probs)\n",
    "    rewards = torch.tensor(\n",
    "        reward_function(prompts=rollout_data[\"repeated_prompts\"], completions=rollout_data[\"formatted_completions\"], answer=rollout_data[\"repeated_answers\"]),\n",
    "        dtype=torch.float32,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    batch_size = rollout_data[\"batch_size\"]\n",
    "    num_generations = rollout_data[\"num_generations\"]\n",
    "    rewards = rewards.view(batch_size, num_generations)\n",
    "    avg_reward = rewards.mean().item()\n",
    "    print(\"Average Reward:\", avg_reward)\n",
    "\n",
    "    mean_rewards = rewards.mean(dim=1).repeat_interleave(num_generations)\n",
    "    std_rewards = rewards.std(dim=1).repeat_interleave(num_generations)\n",
    "    advantages = ((rewards.view(-1) - mean_rewards) / (std_rewards + 1e-4)).unsqueeze(1)\n",
    "    surr1 = ratio * advantages\n",
    "    surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantages\n",
    "    surrogate_loss = torch.min(surr1, surr2)\n",
    "    kl = torch.exp(ref_log_probs - token_log_probs) - (ref_log_probs - token_log_probs) - 1\n",
    "\n",
    "    per_token_loss = surrogate_loss - beta * kl\n",
    "    loss = -((per_token_loss * completion_mask).sum(dim=1) / completion_mask.sum(dim=1)).mean()\n",
    "    return loss, avg_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_grpo(model, \n",
    "                    train_dataloader, \n",
    "                    num_iterations=1, \n",
    "                    num_steps=500, \n",
    "                    num_generations=4, \n",
    "                    max_completion_length=128, \n",
    "                    beta=0.1,\n",
    "                    learning_rate=5e-6, \n",
    "                    mu=3, \n",
    "                    epsilon=0.2, \n",
    "                    reward_function=None, \n",
    "                    device_ids=None):\n",
    "    \"\"\"\n",
    "    Modified GRPO training function that accepts a DataLoader as input.\n",
    "\n",
    "    Args:\n",
    "        model: The language model to train.\n",
    "        train_dataloader (DataLoader): PyTorch DataLoader providing batched training data.\n",
    "        num_iterations (int): Number of outer iterations (reference model updates).\n",
    "        num_steps (int): Number of batch updates per iteration.\n",
    "        num_generations (int): Number of completions per prompt.\n",
    "        max_completion_length (int): Maximum token length for completions.\n",
    "        beta (float): KL penalty coefficient.\n",
    "        learning_rate (float): Learning rate for optimizer.\n",
    "        mu (int): Number of policy updates per batch.\n",
    "        epsilon (float): PPO clipping parameter.\n",
    "        reward_function: Function that calculates rewards for completions.\n",
    "        device_ids (list): List of GPU device IDs for DataParallel.\n",
    "\n",
    "    Returns:\n",
    "        The trained model.\n",
    "\n",
    "    Explanation:\n",
    "        1. For each outer iteration:\n",
    "           - Creates a reference model as a deep copy of the current policy model.\n",
    "           - Reinitializes the optimizer for the policy model.\n",
    "           - Iterates over the DataLoader for num_steps:\n",
    "             a. Retrieves a batch from the DataLoader.\n",
    "             b. Generates rollout data including completions and log probabilities.\n",
    "             c. For mu iterations:\n",
    "                i. Computes the GRPO loss.\n",
    "                ii. Updates the policy model using gradient descent.\n",
    "           - Monitors GPU memory usage and prints progress information.\n",
    "    \"\"\"\n",
    "    # Outer loop: iterative GRPO updates.\n",
    "    for iteration in range(num_iterations):\n",
    "        print(f\"\\nIteration {iteration+1}/{num_iterations}\")\n",
    "\n",
    "        # Create a reference model (deep copy) and set it to eval mode.\n",
    "        ref_model = copy.deepcopy(model)\n",
    "        ref_model.eval()\n",
    "        for param in ref_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        print(\"Reference model created.\")\n",
    "\n",
    "        # Reinitialize the optimizer for this iteration.\n",
    "        optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "        model.train()\n",
    "\n",
    "        # Create an iterator from the DataLoader.\n",
    "        data_iter = iter(train_dataloader)\n",
    "        for step in range(num_steps):\n",
    "            try:\n",
    "                batch_samples = next(data_iter)\n",
    "            except StopIteration:\n",
    "                # Reinitialize the iterator if the DataLoader is exhausted.\n",
    "                data_iter = iter(train_dataloader)\n",
    "                batch_samples = next(data_iter)\n",
    "            \n",
    "            print(batch_samples)\n",
    "\n",
    "            # Generate rollout data without tracking gradients.\n",
    "            with torch.no_grad():\n",
    "                rollout_data = generate_rollout_data(\n",
    "                    model,\n",
    "                    ref_model,\n",
    "                    batch_samples,\n",
    "                    num_generations,\n",
    "                    max_completion_length\n",
    "                )\n",
    "\n",
    "            # Perform multiple policy updates (mu iterations) on the same rollout data.\n",
    "            for grpo_iter in range(mu):\n",
    "                loss, avg_reward = grpo_loss(\n",
    "                    model,\n",
    "                    ref_model,\n",
    "                    rollout_data,\n",
    "                    reward_function,\n",
    "                    beta=beta,\n",
    "                    epsilon=epsilon\n",
    "                )\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
    "                optimizer.step()\n",
    "\n",
    "                # Log training metrics (for example, with wandb)\n",
    "                wandb.log({\n",
    "                    \"loss\": loss.item(),\n",
    "                    \"average_reward\": avg_reward,\n",
    "                    \"iteration\": iteration + 1,\n",
    "                    \"step\": step + 1,\n",
    "                    \"grpo_iter\": grpo_iter + 1\n",
    "                })\n",
    "                print(f\"Iteration {iteration+1}/{num_iterations}, Step {step+1}/{num_steps}, \"\n",
    "                      f\"GRPO iter {grpo_iter+1}/{mu}, loss: {loss.item():.4f}\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    'num_iterations': 1,\n",
    "    'num_steps': 1,\n",
    "    'num_generations': 4, # reduce if you have GPUs with less VRAM\n",
    "    'max_completion_length': 200, # reduce if you have GPUs with less VRAM\n",
    "    'beta': 0.04,\n",
    "    'learning_rate': 5e-6,\n",
    "    'mu': 1,\n",
    "    'epsilon': 0.1\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration 1/1\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 39.49 GiB of which 1.59 GiB is free. Process 1072637 has 28.79 GiB memory in use. Including non-PyTorch memory, this process has 9.10 GiB memory in use. Of the allocated memory 8.62 GiB is allocated by PyTorch, and 4.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_with_grpo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreward_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_reward\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mtraining_config\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 50\u001b[0m, in \u001b[0;36mtrain_with_grpo\u001b[0;34m(model, train_dataloader, num_iterations, num_steps, num_generations, max_completion_length, beta, learning_rate, mu, epsilon, reward_function, device_ids)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00miteration\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_iterations\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Create a reference model (deep copy) and set it to eval mode.\u001b[39;00m\n\u001b[0;32m---> 50\u001b[0m ref_model \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m ref_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m param \u001b[38;5;129;01min\u001b[39;00m ref_model\u001b[38;5;241m.\u001b[39mparameters():\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "    \u001b[0;31m[... skipping similar frames: _deepcopy_dict at line 221 (8 times), deepcopy at line 136 (8 times), _reconstruct at line 259 (3 times), deepcopy at line 162 (3 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:162\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    160\u001b[0m                 y \u001b[38;5;241m=\u001b[39m x\n\u001b[1;32m    161\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 162\u001b[0m                 y \u001b[38;5;241m=\u001b[39m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[0;32m--> 259\u001b[0m         state \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__setstate__\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    261\u001b[0m         y\u001b[38;5;241m.\u001b[39m__setstate__(state)\n",
      "    \u001b[0;31m[... skipping similar frames: _deepcopy_dict at line 221 (1 times), deepcopy at line 136 (1 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:136\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    134\u001b[0m copier \u001b[38;5;241m=\u001b[39m _deepcopy_dispatch\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 136\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:221\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    219\u001b[0m memo[\u001b[38;5;28mid\u001b[39m(x)] \u001b[38;5;241m=\u001b[39m y\n\u001b[1;32m    220\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m--> 221\u001b[0m     y[deepcopy(key, memo)] \u001b[38;5;241m=\u001b[39m \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/copy.py:143\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    141\u001b[0m copier \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__deepcopy__\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    145\u001b[0m     reductor \u001b[38;5;241m=\u001b[39m dispatch_table\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;28mcls\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/rag/lib/python3.12/site-packages/torch/nn/parameter.py:68\u001b[0m, in \u001b[0;36mParameter.__deepcopy__\u001b[0;34m(self, memo)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     67\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)(\n\u001b[0;32m---> 68\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmemory_format\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreserve_format\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_grad\n\u001b[1;32m     69\u001b[0m     )\n\u001b[1;32m     70\u001b[0m     memo[\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m)] \u001b[38;5;241m=\u001b[39m result\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1.96 GiB. GPU 0 has a total capacity of 39.49 GiB of which 1.59 GiB is free. Process 1072637 has 28.79 GiB memory in use. Including non-PyTorch memory, this process has 9.10 GiB memory in use. Of the allocated memory 8.62 GiB is allocated by PyTorch, and 4.16 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "model = train_with_grpo(model, loader,\n",
    "                        reward_function=combined_reward,\n",
    "                        **training_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fe1b2917c20>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
