{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os, sys\n",
    "import wandb\n",
    "import tqdm.notebook as tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from importlib import reload\n",
    "from torch_scatter import scatter\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import preprocess as pp\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset\n",
    "\n",
    "# training imports\n",
    "# from utils.model import load_model, llama_model_path\n",
    "from utils.evaluate import eval_funcs\n",
    "from utils.config import parse_args_llama\n",
    "from utils.ckpt import _save_checkpoint, _reload_best_model\n",
    "from utils.collate import collate_fn\n",
    "from utils.seed import seed_everything\n",
    "from utils.lr_schedule import adjust_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset & Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "data_path = '../data/subgraphs/all'\n",
    "dataset = BiologicalDataset(data_path)\n",
    "idx_split = dataset.get_idx_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets on idx\n",
    "train_dataset = Subset(dataset, idx_split['train'])\n",
    "val_dataset = Subset(dataset, idx_split['val'])\n",
    "test_dataset = Subset(dataset, idx_split['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "batch_size = 4\n",
    "\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=16)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=16)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn,\n",
    "                          num_workers=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70fb5a971c7b4a4d9ddc4916828df7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA!\n",
      "Finished loading LLaMA!\n"
     ]
    }
   ],
   "source": [
    "model = GraphLLM(max_text_len=512,\n",
    "                     max_max_new_tokens=32,\n",
    "                     max_memory=[80, 80],\n",
    "                     llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                     llm_frozen='True',\n",
    "                     revision=\"main\") # args are defaulted in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Graph LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify needed args\n",
    "sys.argv = [''] # needed for argparse in notebooks\n",
    "args = parse_args_llama()\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" # set tokenizers parallelism in RUST to false\n",
    "\n",
    "# set up seed for tracking\n",
    "seed = 42\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4e94a1a76ba4204b6527ab365ed92c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/135460 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving checkpoint at epoch 0 to ../checkpoints/graph_llm_no_text/.\n",
      "Epoch 0/10 | Train Loss (Epoch Mean): 0.10280069005815182 | Validation Loss: 0.09832689589846587 | Best Validation Loss: 0.09832689589846587 at epoch 0\n"
     ]
    }
   ],
   "source": [
    "# options\n",
    "num_training_steps = args.num_epochs * len(train_loader)\n",
    "progress_bar = tqdm.tqdm(range(num_training_steps))\n",
    "best_val_loss = float('inf')\n",
    "save_path = '../checkpoints/graph_llm_no_text/'\n",
    "\n",
    "# set optimizer\n",
    "params = [p for _, p in model.named_parameters() if p.requires_grad] # only update non-frozen params (graph encoder)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [{'params': params, 'lr': args.lr, 'weight_decay': args.wd}, ],\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "## TRAIN LOOP\n",
    "for epoch in range(args.num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss, accum_loss = 0., 0.\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        # clip gradients so large changes don't occur - super small clipping too\n",
    "        clip_grad_norm_(optimizer.param_groups[0]['params'], 0.1)\n",
    "        \n",
    "        # grad steps is a hyprparameter\n",
    "        if (step + 1) % args.grad_steps == 0:\n",
    "            adjust_learning_rate(optimizer.param_groups[0], args.lr, step / len(train_loader) + epoch, args)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        accum_loss += loss.item()\n",
    "\n",
    "        if  (step + 1) % args.grad_steps == 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            # wandb.log({'Lr': lr})\n",
    "            # wandb.log({'Train Loss': accum_loss / args.grad_steps})\n",
    "            accum_loss = 0.\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    # wandb.log({'Train Loss (Epoch Mean)': epoch_loss / len(train_loader)})\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.\n",
    "    eval_output = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            loss = model(batch)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        # wandb.log({'Validation Loss': val_loss})\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        _save_checkpoint(model, optimizer, epoch, args, save_path, is_best=True)\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{args.num_epochs} | Train Loss (Epoch Mean): {epoch_loss / len(train_loader)} | Validation Loss: {val_loss} | Best Validation Loss: {best_val_loss} at epoch {best_epoch}\")\n",
    "\n",
    "    if epoch - best_epoch >= args.patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': [25528, 39773, 45069, 37356],\n",
       " 'pred': ['no<|endoftext|>  ```  No edge between 76 and 118.  ``` 76 -- 118 ```  ``` 76',\n",
       "  'yes<|endoftext|>  no <|endoftext|>yes<|endoftext|>  yes <|endoftext',\n",
       "  'no<|endoftext|>  No edge between nodes 196 and 338.  There is no edge between 196 and 338. ',\n",
       "  'yes<|endoftext|>  Is there an edge between nodes 306 and 98? 306 98  yes <|endoftext'],\n",
       " 'label': ['yes', 'yes', 'yes', 'no'],\n",
       " 'question': ['Is there an edge between nodes 76 and 118?',\n",
       "  'Is there an edge between nodes 470 and 145?',\n",
       "  'Is there an edge between nodes 196 and 338?',\n",
       "  'Is there an edge between nodes 306 and 98?'],\n",
       " 'desc': ['You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Surround your answer in <ans> and </ans> tags. Surround any reasoning with <think> and </think> tags.',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Surround your answer in <ans> and </ans> tags. Surround any reasoning with <think> and </think> tags.',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Surround your answer in <ans> and </ans> tags. Surround any reasoning with <think> and </think> tags.',\n",
       "  'You will be given a biological graph and a question. Provide an answer of YES of NO based on the question and the given input graph. Surround your answer in <ans> and </ans> tags. Surround any reasoning with <think> and </think> tags.']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(test_loader))\n",
    "model.inference(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISMhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [54635, 68094, 4964, 89511], 'pred': ['no<|endoftext|>yes<|endoftext|>no<|endoftext|>yes<|endoftext|>', 'yes<|endoftext|>no<|endoftext|>yes<|endoftext|>yes<|endoftext|>', 'yes<|endoftext|>no<|endoftext|>yes<|endoftext|>yes<|endoftext|>', 'yes<|endoftext|>no<|endoftext|>yes<|endoftext|>yes<|endoftext|>'], 'label': ['no', 'yes', 'yes', 'yes'], 'question': ['Is there an edge between nodes 494 and 33?', 'Is there an edge between nodes 157 and 433?', 'Is there an edge between nodes 297 and 62?', 'Is there an edge between nodes 204 and 245?'], 'desc': ['You will be given a biological graph and a question. You need to determine the answer to the question based on the graph.', 'You will be given a biological graph and a question. You need to determine the answer to the question based on the graph.', 'You will be given a biological graph and a question. You need to determine the answer to the question based on the graph.', 'You will be given a biological graph and a question. You need to determine the answer to the question based on the graph.']}\n"
     ]
    }
   ],
   "source": [
    "# test inference on one batch\n",
    "batch = next(iter(test_loader))\n",
    "out = model.inference(batch)\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
