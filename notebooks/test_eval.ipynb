{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import wandb\n",
    "import tqdm.notebook as tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from importlib import reload\n",
    "from torch_scatter import scatter\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import preprocess as pp\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset\n",
    "\n",
    "# training imports\n",
    "from utils.evaluate import eval_funcs, normalize\n",
    "from utils.config import parse_args_llama\n",
    "from utils.ckpt import _save_checkpoint, _reload_best_model\n",
    "from utils.collate import collate_fn\n",
    "from utils.seed import seed_everything\n",
    "from utils.lr_schedule import adjust_learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "data_path = '../data/DREAM4_gold_standards/connections_node_label'\n",
    "dataset = BiologicalDataset(data_path)\n",
    "idx_split = dataset.get_idx_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DGX01/Personal/krusepi/codebase/projects/llms/JAIL-RAG/notebooks/../utils/bio_graphs.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph = torch.load(f'{self.path}/graphs/{index}.pt')\n"
     ]
    }
   ],
   "source": [
    "# split datasets on idx\n",
    "train_dataset = [dataset[i] for i in idx_split[\"train\"]]\n",
    "val_dataset = [dataset[i] for i in idx_split[\"val\"]]\n",
    "test_dataset = [dataset[i] for i in idx_split[\"test\"]]\n",
    "\n",
    "# options\n",
    "batch_size = 12\n",
    "\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b69ec3b1287344e1a071030070de0079",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA...\n",
      "Finished loading LLaMA...\n"
     ]
    }
   ],
   "source": [
    "model = LLM(max_text_len=512,\n",
    "                  max_max_new_tokens=32,\n",
    "                  max_memory=[80, 80],\n",
    "                  llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                #   llm_model_path='meta-llama/Llama-2-7B-chat-hf',\n",
    "                  llm_frozen='True',\n",
    "                  revision=\"main\") # need to add args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# make prediction\n",
    "batch = next(iter(train_loader))\n",
    "out = model.inference(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yes according to graph there is edge between g8 and g10 edge is induced by edge g2 is associated with g\n",
      "no \n",
      "\n",
      "yes according to graph there is edge between g7 and g4 edge is associated with layer coexpressionheart\n",
      "yes \n",
      "\n",
      "Correct!\n",
      "\n",
      "yes there is edge between nodes g9 and g5 edge is associated with relation g1 is associated with g5\n",
      "no \n",
      "\n",
      "yes there is edge between nodes g5 and g1 because g1 is associated with g5\n",
      "yes \n",
      "\n",
      "Correct!\n",
      "\n",
      "yes there is edge between nodes g3 and g1 as indicated by statement g1 is associated with g3 this is und\n",
      "yes \n",
      "\n",
      "Correct!\n",
      "\n",
      "yes there is edge between g1 and g5 which is also between g1 and g4 and g4 and g5 so\n",
      "no \n",
      "\n",
      "there are no edges between g5 and g9 edges are defined in following format nodeid x is gy where x is\n",
      "no \n",
      "\n",
      "Correct!\n",
      "\n",
      "there is edge between nodes g8 and g3 edge is result of association g2 is associated with g8 and g\n",
      "no \n",
      "\n",
      "yes there is edge between nodes g6 and g3 edge is associated with relationship g3 is associated with g6\n",
      "no \n",
      "\n",
      "yes there is edge between nodes g3 and g7 edge is g3 is associated with g7endoftext\n",
      "yes \n",
      "\n",
      "Correct!\n",
      "\n",
      "yes there is edge between nodes g6 and g8 as stated in edge list g2 is associated with g6 g\n",
      "yes \n",
      "\n",
      "Correct!\n",
      "\n",
      "yes there is edge between g4 and g3 this is indicated by statement g3 is associated with g4endoft\n",
      "yes \n",
      "\n",
      "Correct!\n",
      "\n",
      "Accuracy: 58.33%\n"
     ]
    }
   ],
   "source": [
    "    pred = out['pred']\n",
    "    actual = out['label']\n",
    "\n",
    "    n_correct = 0\n",
    "    for p, a in zip(pred, actual):\n",
    "        p = normalize(p)\n",
    "        a = normalize(a) + ' '\n",
    "        print(p)\n",
    "        print(a)\n",
    "        print()\n",
    "        if a in p:\n",
    "            n_correct += 1\n",
    "            print(\"Correct!\")\n",
    "            print()\n",
    "    acc = n_correct / len(pred)\n",
    "    print(f\"Accuracy: {acc:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
