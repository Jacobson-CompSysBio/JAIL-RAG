{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import wandb\n",
    "import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from importlib import reload\n",
    "from torch_scatter import scatter\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import preprocess as pp\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset\n",
    "\n",
    "# training imports\n",
    "# from utils.model import load_model, llama_model_path\n",
    "from utils.evaluate import eval_funcs\n",
    "from utils.config import parse_args_llama\n",
    "from utils.ckpt import _save_checkpoint, _reload_best_model\n",
    "from utils.collate import collate_fn\n",
    "from utils.seed import seed_everything\n",
    "from utils.lr_schedule import adjust_learning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Make Dataloader\n",
    "* dataloader returns dict with keys `[\"ids\"]`, `[\"question\"]`, `[\"label\"]`, `[\"desc\"]`, `[\"graph\"]`\n",
    "* ask ken if our dataset setup is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "data_path = '../data/DREAM4_gold_standards/connections_node_label'\n",
    "dataset = BiologicalDataset(data_path)\n",
    "idx_split = dataset.get_idx_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split datasets on idx\n",
    "train_dataset = [dataset[i] for i in idx_split[\"train\"]]\n",
    "val_dataset = [dataset[i] for i in idx_split[\"val\"]]\n",
    "test_dataset = [dataset[i] for i in idx_split[\"test\"]]\n",
    "\n",
    "# options\n",
    "batch_size = 2\n",
    "\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load In Encoder + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d258ba2c9a34024bc3e88a1a6c712e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA...\n",
      "Finished loading LLaMA...\n"
     ]
    }
   ],
   "source": [
    "vanilla_llm = LLM(max_text_len=512,\n",
    "                  max_max_new_tokens=32,\n",
    "                  max_memory=[80, 80],\n",
    "                  llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                #   llm_model_path='meta-llama/Llama-2-7B-chat-hf',\n",
    "                  llm_frozen='True',\n",
    "                  revision=\"main\") # need to add args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41eca932fee34ca89f1f523f28b62ee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA!\n",
      "Finished loading LLaMA!\n"
     ]
    }
   ],
   "source": [
    "graph_llm = GraphLLM(max_text_len=512,\n",
    "                     max_max_new_tokens=32,\n",
    "                     max_memory=[80, 80],\n",
    "                     llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                     llm_frozen='True',\n",
    "                     revision=\"main\") # args are defaulted in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Perform Initial Untrained Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [28, 21], 'question': ['Is there an edge between nodes G8 and G10?', 'Is there an edge between nodes G4 and G2?'], 'label': ['no', 'no'], 'desc': [\"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\", \"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\"], 'graph': DataBatch(x=[20, 1024], edge_index=[2, 26], num_nodes=20, batch=[20], ptr=[3])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': [28, 21],\n",
       " 'pred': [\"Yes, there is an edge between nodes G8 and G10. The edge is implied by the statement 'G3 is associated with G10', since G\",\n",
       "  \"Yes, there is an edge between nodes G4 and G2. The edge is associated with the relationship 'G1 is associated with G2'. This relationship\"],\n",
       " 'label': ['no', 'no'],\n",
       " 'question': ['Is there an edge between nodes G8 and G10?',\n",
       "  'Is there an edge between nodes G4 and G2?'],\n",
       " 'desc': [\"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\",\n",
       "  \"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\"]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_llm.inference(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': [28, 21],\n",
       " 'pred': [\"Yes, there is an edge between nodes G8 and G10. This edge is labeled with the association 'G6 is associated with G8'. The edge\",\n",
       "  ' No, there is no edge between nodes G4 and G2. The graph only has edges between nodes G1 and G2, G1 and G3'],\n",
       " 'label': ['no', 'no'],\n",
       " 'question': ['Is there an edge between nodes G8 and G10?',\n",
       "  'Is there an edge between nodes G4 and G2?'],\n",
       " 'desc': [\"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\",\n",
       "  \"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\"]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "graph_llm.inference(batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name MODEL_NAME]\n",
      "                             [--project PROJECT] [--seed SEED]\n",
      "                             [--dataset DATASET] [--lr LR] [--wd WD]\n",
      "                             [--patience PATIENCE] [--batch_size BATCH_SIZE]\n",
      "                             [--grad_steps GRAD_STEPS]\n",
      "                             [--num_epochs NUM_EPOCHS]\n",
      "                             [--warmup_epochs WARMUP_EPOCHS]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--llm_model_name LLM_MODEL_NAME]\n",
      "                             [--llm_model_path LLM_MODEL_PATH]\n",
      "                             [--llm_frozen LLM_FROZEN]\n",
      "                             [--llm_num_virtual_tokens LLM_NUM_VIRTUAL_TOKENS]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--max_txt_len MAX_TXT_LEN]\n",
      "                             [--max_new_tokens MAX_NEW_TOKENS]\n",
      "                             [--max_memory MAX_MEMORY]\n",
      "                             [--gnn_model_name GNN_MODEL_NAME]\n",
      "                             [--gnn_num_layers GNN_NUM_LAYERS]\n",
      "                             [--gnn_in_dim GNN_IN_DIM]\n",
      "                             [--gnn_hidden_dim GNN_HIDDEN_DIM]\n",
      "                             [--gnn_num_heads GNN_NUM_HEADS]\n",
      "                             [--gnn_dropout GNN_DROPOUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/pkr/.local/share/jupyter/runtime/kernel-v2-3257950TA4eTjJEiqnD.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rag/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# specify needed args\n",
    "args = parse_args_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up wandb, seed for tracking\n",
    "seed = 42\n",
    "wandb.init(project=f\"{project}\",\n",
    "            name=f\"{dataset}_{model_name}_seed{seed}\",\n",
    "            config=args)\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# set optimizer\n",
    "params = [p for _, p in model.named_parameters() if p.requires_grad] # only update non-frozen params (graph encoder)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [{'params': params, 'lr': lr, 'weight_decay': wd}, ],\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "## TRAIN LOOP\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss, accum_loss = 0., 0.\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch)\n",
    "        \n",
    "        # clip gradients so large changes don't occur - super small clipping too\n",
    "        clip_grad_norm_(optimizer.param_groups[0]['params'], 0.1)\n",
    "        \n",
    "        # grad steps is a hyprparameter\n",
    "        if (step + 1) % grad_steps == 0:\n",
    "            adjust_learning_rate(optimizer.param_groups[0], lr, step / len(train_loader) + epoch)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss, accum_loss += loss.item(), loss.item()\n",
    "\n",
    "        if  (step + 1) % grad_steps == 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            wandb.log({'Lr': lr})\n",
    "            wandb.log({'Train Loss': accum_loss / grad_steps})\n",
    "            accum_loss = 0.\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Train Loss (Epoch Mean): {epoch_loss / len(train_loader)}\")\n",
    "    wandb.log({'Train Loss (Epoch Mean)': epoch_loss / len(train_loader)})\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.\n",
    "    eval_output = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            loss = model(batch)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | Validation Loss: {val_loss}\")\n",
    "        wandb.log({'Validation Loss': val_loss})\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        _save_checkpoint(model, optimizer, epoch, args, is_best=True)\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Best Validation Loss: {best_val_loss} at epoch {best_epoch}\")\n",
    "\n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "model = _reload_best_model(model, args)\n",
    "model.eval()\n",
    "\n",
    "progress_bar_test = tqdm(range(len(test_loader)))\n",
    "with open(path, \"w\") as f:\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model.inference(batch)\n",
    "            df = pd.DataFrame(output)\n",
    "            for _, row in df.iterrows():\n",
    "                f.write(json.dumps(dict(row)) + \"\\n\")\n",
    "        progress_bar_test.update(1)\n",
    "\n",
    "# post process + compute metrics\n",
    "acc = eval_funcs[dataset](path)\n",
    "print(f'Test Acc: {acc}')\n",
    "wandb.log({'Test Acc': acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
