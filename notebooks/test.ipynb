{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import wandb\n",
    "import tqdm\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from importlib import reload\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import preprocess as pp\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.bio_graphs import BiologicalDataset\n",
    "\n",
    "# training imports\n",
    "# from utils.model import load_model, llama_model_path\n",
    "from utils.evaluate import eval_funcs\n",
    "from utils.config import parse_args_llama\n",
    "from utils.ckpt import _save_checkpoint, _reload_best_model\n",
    "from utils.collate import collate_fn\n",
    "from utils.seed import seed_everything\n",
    "from utils.lr_schedule import adjust_learning_rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Networks as `Multiplex` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist_name = '../data/DREAM4_gold_standards/mono_flist.tsv'\n",
    "mp = Multiplex(flist_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Textualize Graphs (Ken's Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1 is associated with G2\n",
      "G1 is associated with G3\n",
      "G1 is associated with G4\n",
      "G1 is associated with G5\n",
      "G2 is associated with G6\n",
      "G2 is associated with G8\n",
      "G3 is associated with G4\n",
      "G3 is associated with G7\n",
      "G3 is associated with G10\n",
      "G4 is associated with G7\n",
      "G4 is associated with G10\n",
      "G6 is associated with G8\n",
      "G9 is associated with G10\n",
      "node_id 0 is G1\n",
      "node_id 1 is G10\n",
      "node_id 2 is G2\n",
      "node_id 3 is G3\n",
      "node_id 4 is G4\n",
      "node_id 5 is G5\n",
      "node_id 6 is G6\n",
      "node_id 7 is G7\n",
      "node_id 8 is G8\n",
      "node_id 9 is G9\n",
      "layer 0 is from coexpression-heart\n"
     ]
    }
   ],
   "source": [
    "textualize = load_textualizer['all']\n",
    "graph_text = textualize(mp)\n",
    "\n",
    "# view first 10 items\n",
    "for i in range(len(graph_text)):\n",
    "    print(graph_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Make Dataloader\n",
    "* dataloader returns dict with keys `[\"ids\"]`, `[\"question\"]`, `[\"label\"]`, `[\"desc\"]`, `[\"graph\"]`\n",
    "* ask ken if our dataset setup is the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get dataset\n",
    "data_path = '../data/DREAM4_gold_standards/connections_node_id'\n",
    "dataset = BiologicalDataset(data_path)\n",
    "idx_split = dataset.get_idx_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/DGX01/Personal/krusepi/codebase/projects/llms/JAIL-RAG/notebooks/../utils/bio_graphs.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  graph = torch.load(f'{self.path}/graphs/{index}.pt')\n"
     ]
    }
   ],
   "source": [
    "# split datasets on idx\n",
    "train_dataset = [dataset[i] for i in idx_split[\"train\"]]\n",
    "val_dataset = [dataset[i] for i in idx_split[\"val\"]]\n",
    "test_dataset = [dataset[i] for i in idx_split[\"test\"]]\n",
    "\n",
    "# options\n",
    "batch_size = 1\n",
    "\n",
    "# make dataloaders\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                          batch_size=batch_size,\n",
    "                          drop_last=True,\n",
    "                          pin_memory=True,\n",
    "                          shuffle=True,\n",
    "                          collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load In Encoder + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "614dc7ebfc1c4bdc8cad6ff7c5a00946",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA...\n",
      "Finished loading LLaMA...\n"
     ]
    }
   ],
   "source": [
    "vanilla_llm = LLM(max_text_len=512,\n",
    "                  max_max_new_tokens=32,\n",
    "                  max_memory=[80, 80],\n",
    "                  llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                #   llm_model_path='meta-llama/Llama-2-7B-chat-hf',\n",
    "                  llm_frozen='True',\n",
    "                  revision=\"main\") # need to add args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09e78b612e1944b496761c10a07a13c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA!\n",
      "Finished loading LLaMA!\n"
     ]
    }
   ],
   "source": [
    "graph_llm = GraphLLM(max_text_len=512,\n",
    "                     max_max_new_tokens=32,\n",
    "                     max_memory=[80, 80],\n",
    "                     llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                     llm_frozen='True',\n",
    "                     revision=\"main\") # args are defaulted in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Perform Initial Untrained Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': [1], 'question': ['Is there an edge between nodes 4 and 0?'], 'label': ['yes'], 'desc': [\"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\"], 'graph': DataBatch(x=[1], edge_index=[2, 13], num_nodes=10, batch=[10], ptr=[2])}\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(train_loader))\n",
    "print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'id': [1],\n",
       " 'pred': [\" 1\\ns>[INST]['yes', 'no'][INST] 1\\ns>[INST]['yes', 'yes'][INST] 1\"],\n",
       " 'label': ['yes'],\n",
       " 'question': ['Is there an edge between nodes 4 and 0?'],\n",
       " 'desc': [\"['G1 is associated with G2', 'G1 is associated with G3', 'G1 is associated with G4', 'G1 is associated with G5', 'G2 is associated with G6', 'G2 is associated with G8', 'G3 is associated with G4', 'G3 is associated with G7', 'G3 is associated with G10', 'G4 is associated with G7', 'G4 is associated with G10', 'G6 is associated with G8', 'G9 is associated with G10', 'node_id 0 is G1', 'node_id 1 is G10', 'node_id 2 is G2', 'node_id 3 is G3', 'node_id 4 is G4', 'node_id 5 is G5', 'node_id 6 is G6', 'node_id 7 is G7', 'node_id 8 is G8', 'node_id 9 is G9', 'layer 0 is from coexpression-heart']\"]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_llm.inference(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LlamaModel.get_input_embeddings() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mgraph_llm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/DGX01/Personal/krusepi/codebase/projects/llms/JAIL-RAG/notebooks/../utils/graph_llm.py:215\u001b[0m, in \u001b[0;36mGraphLLM.inference\u001b[0;34m(self, samples)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# encode special tokens\u001b[39;00m\n\u001b[1;32m    214\u001b[0m eos_user_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(EOS_USER, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 215\u001b[0m bos_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mword_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mBOS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m pad_embeds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding(torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mpad_token_id))\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    218\u001b[0m \u001b[38;5;66;03m# encode graphs\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: LlamaModel.get_input_embeddings() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "graph_llm.inference(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s>\\nYes, there is a path between nodes G9 and G5. The path is: G1 -> G2 -> G5 or G1 ->'] ['yes']\n"
     ]
    }
   ],
   "source": [
    "print(out['pred'], out['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--model_name MODEL_NAME]\n",
      "                             [--project PROJECT] [--seed SEED]\n",
      "                             [--dataset DATASET] [--lr LR] [--wd WD]\n",
      "                             [--patience PATIENCE] [--batch_size BATCH_SIZE]\n",
      "                             [--grad_steps GRAD_STEPS]\n",
      "                             [--num_epochs NUM_EPOCHS]\n",
      "                             [--warmup_epochs WARMUP_EPOCHS]\n",
      "                             [--eval_batch_size EVAL_BATCH_SIZE]\n",
      "                             [--llm_model_name LLM_MODEL_NAME]\n",
      "                             [--llm_model_path LLM_MODEL_PATH]\n",
      "                             [--llm_frozen LLM_FROZEN]\n",
      "                             [--llm_num_virtual_tokens LLM_NUM_VIRTUAL_TOKENS]\n",
      "                             [--output_dir OUTPUT_DIR]\n",
      "                             [--max_txt_len MAX_TXT_LEN]\n",
      "                             [--max_new_tokens MAX_NEW_TOKENS]\n",
      "                             [--max_memory MAX_MEMORY]\n",
      "                             [--gnn_model_name GNN_MODEL_NAME]\n",
      "                             [--gnn_num_layers GNN_NUM_LAYERS]\n",
      "                             [--gnn_in_dim GNN_IN_DIM]\n",
      "                             [--gnn_hidden_dim GNN_HIDDEN_DIM]\n",
      "                             [--gnn_num_heads GNN_NUM_HEADS]\n",
      "                             [--gnn_dropout GNN_DROPOUT]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: --f=/home/pkr/.local/share/jupyter/runtime/kernel-v2-3257950TA4eTjJEiqnD.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pkr/miniconda3/envs/rag/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# specify needed args\n",
    "args = parse_args_llama()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up wandb, seed for tracking\n",
    "seed = 42\n",
    "wandb.init(project=f\"{project}\",\n",
    "            name=f\"{dataset}_{model_name}_seed{seed}\",\n",
    "            config=args)\n",
    "seed_everything(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "# set optimizer\n",
    "params = [p for _, p in model.named_parameters() if p.requires_grad] # only update non-frozen params (graph encoder)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [{'params': params, 'lr': lr, 'weight_decay': wd}, ],\n",
    "    betas=(0.9, 0.95)\n",
    ")\n",
    "\n",
    "## TRAIN LOOP\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss, accum_loss = 0., 0.\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch)\n",
    "        \n",
    "        # clip gradients so large changes don't occur - super small clipping too\n",
    "        clip_grad_norm_(optimizer.param_groups[0]['params'], 0.1)\n",
    "        \n",
    "        # grad steps is a hyprparameter\n",
    "        if (step + 1) % grad_steps == 0:\n",
    "            adjust_learning_rate(optimizer.param_groups[0], lr, step / len(train_loader) + epoch)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss, accum_loss += loss.item(), loss.item()\n",
    "\n",
    "        if  (step + 1) % grad_steps == 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            wandb.log({'Lr': lr})\n",
    "            wandb.log({'Train Loss': accum_loss / grad_steps})\n",
    "            accum_loss = 0.\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Train Loss (Epoch Mean): {epoch_loss / len(train_loader)}\")\n",
    "    wandb.log({'Train Loss (Epoch Mean)': epoch_loss / len(train_loader)})\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.\n",
    "    eval_output = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            loss = model(batch)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | Validation Loss: {val_loss}\")\n",
    "        wandb.log({'Validation Loss': val_loss})\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        _save_checkpoint(model, optimizer, epoch, args, is_best=True)\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Best Validation Loss: {best_val_loss} at epoch {best_epoch}\")\n",
    "\n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "model = _reload_best_model(model, args)\n",
    "model.eval()\n",
    "\n",
    "progress_bar_test = tqdm(range(len(test_loader)))\n",
    "with open(path, \"w\") as f:\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model.inference(batch)\n",
    "            df = pd.DataFrame(output)\n",
    "            for _, row in df.iterrows():\n",
    "                f.write(json.dumps(dict(row)) + \"\\n\")\n",
    "        progress_bar_test.update(1)\n",
    "\n",
    "# post process + compute metrics\n",
    "acc = eval_funcs[dataset](path)\n",
    "print(f'Test Acc: {acc}')\n",
    "wandb.log({'Test Acc': acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
