{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "import transformers\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "sys.path.append('../')\n",
    "\n",
    "from utils import preprocess as pp\n",
    "# from utils.llm import llm\n",
    "from utils.graph_llm import GraphLLM\n",
    "from utils.llm import LLM\n",
    "from utils.multiplex import Multiplex\n",
    "from utils.textualize import *\n",
    "from utils.GetFileNames import GetFileNames\n",
    "from utils.GetLowestGPU import GetLowestGPU\n",
    "from utils.bio_graphs import BiologicalDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 1: Load Networks as `Multiplex` object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "flist_name = '../data/DREAM4_gold_standards/flist.tsv'\n",
    "mp = Multiplex(flist_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 2: Textualize Graphs (Ken's Code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G1 is associated with G2\n",
      "G1 is associated with G3\n",
      "G1 is associated with G4\n",
      "G1 is associated with G5\n",
      "G1 is associated with G6\n",
      "G1 is associated with G7\n",
      "G1 is associated with G8\n",
      "G1 is associated with G9\n",
      "G1 is associated with G10\n",
      "G2 is associated with G6\n"
     ]
    }
   ],
   "source": [
    "textualize = load_textualizer['edges']\n",
    "graph_text = textualize(mp)\n",
    "\n",
    "# view first 10 items\n",
    "for i in range(10):\n",
    "    print(graph_text[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Make Dataloader\n",
    "* dataloader returns dict with keys `[\"ids\"]`, `[\"desc\"]`,`[\"question\"]`,`[\"label\"]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_data = BiologicalDataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "bio_dataloader = DataLoader(bio_data, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Load In Encoder + LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63b704180829455599219a1c2d8693d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA...\n",
      "Finished loading LLaMA...\n",
      "Loading LLaMA...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "251044fb153243178ca43011c58450be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freezing LLaMA!\n",
      "Finished loading LLaMA!\n"
     ]
    }
   ],
   "source": [
    "vanilla_llm = LLM(max_text_len=512,\n",
    "                  max_max_new_tokens=32,\n",
    "                  max_memory=[80, 80],\n",
    "                  llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                  llm_frozen='True',\n",
    "                  revision=\"main\") # need to add args\n",
    "\n",
    "graph_llm = GraphLLM(max_text_len=512,\n",
    "                     max_max_new_tokens=32,\n",
    "                     max_memory=[80, 80],\n",
    "                     llm_model_path='meta-llama/Meta-Llama-3-8B-Instruct',\n",
    "                     llm_frozen='True',\n",
    "                     revision=\"main\") # args are defaulted in the class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Perform Initial Untrained Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(bio_dataloader))\n",
    "out = vanilla_llm.inference(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['s>\\nYes, there is a path between nodes G9 and G5. The path is: G1 -> G2 -> G5 or G1 ->'] ['yes']\n"
     ]
    }
   ],
   "source": [
    "print(out['pred'], out['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 6: Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# options\n",
    "num_training_steps = num_epochs * len(train_loader)\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "best_val_loss = float('inf')\n",
    "\n",
    "## TRAIN LOOP\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model.train()\n",
    "    epoch_loss, accum_loss = 0., 0.\n",
    "\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch)\n",
    "        \n",
    "        # clip gradients so large changes don't occur\n",
    "        clip_grad_norm_(optimizer.param_groups[0]['params'], 0.1)\n",
    "        \n",
    "        # grad steps is a hyprparameter\n",
    "        if (step + 1) % grad_steps == 0:\n",
    "            adjust_learning_rate(optimizer.param_groups[0], lr, step / len(train_loader) + epoch)\n",
    "        \n",
    "        optimizer.step()\n",
    "        epoch_loss, accum_loss += loss.item(), loss.item()\n",
    "\n",
    "        if  (step + 1) % grad_steps == 0:\n",
    "            lr = optimizer.param_groups[0]['lr']\n",
    "            wandb.log({'Lr': lr})\n",
    "            wandb.log({'Train Loss': accum_loss / grad_steps})\n",
    "            accum_loss = 0.\n",
    "        \n",
    "        progress_bar.update(1)\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Train Loss (Epoch Mean): {epoch_loss / len(train_loader)}\")\n",
    "    wandb.log({'Train Loss (Epoch Mean)': epoch_loss / len(train_loader)})\n",
    "\n",
    "    # validation\n",
    "    val_loss = 0.\n",
    "    eval_output = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for step, batch in enumerate(val_loader):\n",
    "            loss = model(batch)\n",
    "            val_loss += loss.item()\n",
    "        val_loss /= len(val_loader)\n",
    "        print(f\"Epoch {epoch}/{num_epochs} | Validation Loss: {val_loss}\")\n",
    "        wandb.log({'Validation Loss': val_loss})\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        _save_checkpoint(model, optimizer, epoch, args, is_best=True\n",
    "        best_epoch = epoch\n",
    "    \n",
    "    print(f\"Epoch {epoch}/{num_epochs} | Best Validation Loss: {best_val_loss} at epoch {best_epoch}\")\n",
    "\n",
    "    if epoch - best_epoch >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_max_memory_allocated()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 7: Evaluate After Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval\n",
    "model = _reload_best_model(model, args)\n",
    "model.eval()\n",
    "\n",
    "progress_bar_test = tqdm(range(len(test_loader)))\n",
    "with open(path, \"w\") as f:\n",
    "    for step, batch in enumerate(test_loader):\n",
    "        with torch.no_grad():\n",
    "            output = model.inference(batch)\n",
    "            df = pd.DataFrame(output)\n",
    "            for _, row in df.iterrows():\n",
    "                f.write(json.dumps(dict(row)) + \"\\n\")\n",
    "        progress_bar_test.update(1)\n",
    "\n",
    "# post process + compute metrics\n",
    "acc = eval_funcs[dataset](path)\n",
    "print(f'Test Acc: {acc}')\n",
    "wandb.log({'Test Acc': acc})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
